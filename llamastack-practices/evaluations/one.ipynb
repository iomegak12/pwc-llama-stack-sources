{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
    "from termcolor import colored\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import json\n",
    "from rich.pretty import pprint\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ[\"TOGETHER_URL\"]\n",
    "together_api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=host,\n",
    "    provider_data={\n",
    "        \"tavily_search_api_key\": os.getenv(\"TAVILY_SEARCH_API_KEY\"),\n",
    "        \"together_api_key\": together_api_key,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    instructions=\"You are a helpful assistant. Use web_search tool to answer the questions.\",\n",
    "    tools=[\"builtin::websearch\"],\n",
    ")\n",
    "user_prompts = [\n",
    "    \"Which teams played in the NBA western conference finals of 2024. Search the web for the answer.\",\n",
    "    \"In which episode and season of South Park does Bill Cosby (BSM-471) first appear? Give me the number and title. Search the web for the answer.\",\n",
    "    \"What is the British-American kickboxer Andrew Tate's kickboxing name? Search the web for the answer.\",\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(uuid4().hex)\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=agent.agent_id,\n",
    ")\n",
    "\n",
    "pprint(session_response.turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tool_call = 0\n",
    "for turn in session_response.turns:\n",
    "    for step in turn.steps:\n",
    "        if step.step_type == \"tool_execution\" and step.tool_calls[0].tool_name == \"brave_search\":\n",
    "            num_tool_call += 1\n",
    "\n",
    "print(f\"{num_tool_call}/{len(session_response.turns)} user prompts are followed by a tool call to `brave_search`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = []\n",
    "\n",
    "expected_answers = [\n",
    "    \"Dallas Mavericks and the Minnesota Timberwolves\",\n",
    "    \"Season 4, Episode 12\",\n",
    "    \"King Cobra\",\n",
    "]\n",
    "\n",
    "for i, turn in enumerate(session_response.turns):\n",
    "    eval_rows.append(\n",
    "        {\n",
    "            \"input_query\": turn.input_messages[0].content,\n",
    "            \"generated_answer\": turn.output_message.content,\n",
    "            \"expected_answer\": expected_answers[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "pprint(eval_rows)\n",
    "\n",
    "scoring_params = {\n",
    "    \"basic::subset_of\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "pprint(scoring_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting traces for session_id={session_id}\")\n",
    "import json\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "agent_logs = []\n",
    "\n",
    "for span in client.telemetry.query_spans(\n",
    "    attribute_filters=[\n",
    "        {\"key\": \"session_id\", \"op\": \"eq\", \"value\": session_id},\n",
    "    ],\n",
    "    attributes_to_return=[\"input\", \"output\"],\n",
    "):\n",
    "    if span.attributes[\"output\"] != \"no shields\":\n",
    "        agent_logs.append(span.attributes)\n",
    "\n",
    "print(\"Here are examples of traces:\")\n",
    "pprint(agent_logs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "eval_rows = []\n",
    "\n",
    "for log in agent_logs:\n",
    "    input = json.loads(log[\"input\"])\n",
    "    if isinstance(input, list):\n",
    "        input = input[-1]\n",
    "    if input[\"role\"] == \"user\":\n",
    "        eval_rows.append(\n",
    "            {\n",
    "                \"input_query\": input[\"content\"],\n",
    "                \"generated_answer\":  log[\"output\"],\n",
    "                # check if generated_answer uses tools brave_search\n",
    "                \"expected_answer\": \"brave_search\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "pprint(eval_rows)\n",
    "\n",
    "scoring_params = {\n",
    "    \"basic::subset_of\": None,\n",
    "}\n",
    "\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "\n",
    "pprint(scoring_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "from rich.pretty import pprint\n",
    "\n",
    "judge_model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "Given a QUESTION and GENERATED_RESPONSE and EXPECTED_RESPONSE.\n",
    "\n",
    "Compare the factual content of the GENERATED_RESPONSE with the EXPECTED_RESPONSE. Ignore any differences in style, grammar, or punctuation.\n",
    "  The GENERATED_RESPONSE may either be a subset or superset of the EXPECTED_RESPONSE, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "  (A) The GENERATED_RESPONSE is a subset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (B) The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (C) The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE.\n",
    "  (D) There is a disagreement between the GENERATED_RESPONSE and the EXPECTED_RESPONSE.\n",
    "  (E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Give your answer in the format \"Answer: One of ABCDE, Explanation: \".\n",
    "\n",
    "Your actual task:\n",
    "\n",
    "QUESTION: {input_query}\n",
    "GENERATED_RESPONSE: {generated_answer}\n",
    "EXPECTED_RESPONSE: {expected_answer}\n",
    "\"\"\"\n",
    "\n",
    "input_query = (\n",
    "    \"What are the top 5 topics that were explained? Only list succinct bullet points.\"\n",
    ")\n",
    "generated_answer = \"\"\"\n",
    "Here are the top 5 topics that were explained in the documentation for Torchtune:\n",
    "\n",
    "* What is LoRA and how does it work?\n",
    "* Fine-tuning with LoRA: memory savings and parameter-efficient finetuning\n",
    "* Running a LoRA finetune with Torchtune: overview and recipe\n",
    "* Experimenting with different LoRA configurations: rank, alpha, and attention modules\n",
    "* LoRA finetuning\n",
    "\"\"\"\n",
    "expected_answer = \"\"\"LoRA\"\"\"\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"input_query\": input_query,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"expected_answer\": expected_answer,\n",
    "    },\n",
    "]\n",
    "\n",
    "scoring_params = {\n",
    "    \"llm-as-judge::base\": {\n",
    "        \"judge_model\": judge_model_id,\n",
    "        \"prompt_template\": JUDGE_PROMPT,\n",
    "        \"type\": \"llm_as_judge\",\n",
    "        \"judge_score_regexes\": [\"Answer: (A|B|C|D|E)\"],\n",
    "    },\n",
    "    \"basic::subset_of\": None,\n",
    "}\n",
    "\n",
    "response = client.scoring.score(\n",
    "    input_rows=rows, \n",
    "    scoring_functions=scoring_params)\n",
    "\n",
    "pprint(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
