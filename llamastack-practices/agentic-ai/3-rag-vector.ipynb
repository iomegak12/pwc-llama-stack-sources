{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with Vector Database - Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger, RAGDocument\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = f\"http://{os.environ[\"LLAMA_STACK_SERVER_REMOTE_HOST\"]}:{os.environ[\"LLAMA_STACK_SERVER_REMOTE_PORT\"]}\"\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"datasets.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\"\n",
    "]\n",
    "\n",
    "base_document_url = \"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/\"\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content = f\"{base_document_url}/{url}\",\n",
    "        mime_type = \"text/plain\",\n",
    "        metadata = {}\n",
    "    )\n",
    "    \n",
    "    for i, url in enumerate(urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "\n",
    "vector_providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_shields = [shield.identifier for shield in client.shields.list()]\n",
    "\n",
    "if not available_shields:\n",
    "    print(colored(\"No Shields are available\", \"yellow\"))\n",
    "else:\n",
    "    print(f\"Available Sheilds: {available_shields}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = [model.identifier for model in client.models.list() if model.model_type == \"llm\"]\n",
    "\n",
    "print(f\"Available Models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vector_provider = vector_providers[0]\n",
    "selected_model = available_models[0]\n",
    "selected_shields = available_shields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vector_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_id = f\"test-vector-db_{uuid4()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_embedding_models = [model.identifier for model in client.models.list() if model.model_type == \"embedding\"]\n",
    "\n",
    "print(f\"Available Models: {available_embedding_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=available_embedding_models[0],\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_tool_groups = client.toolgroups.list()\n",
    "\n",
    "print(available_tool_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "    You are a helpful assistant, who answers questions professionally. \n",
    "    Use Knowledge Search tool to gather information needed to answer the question.\n",
    "    Answer very succintly.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=selected_model,\n",
    "    instructions=instructions,\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.2,\n",
    "        \"type\": \"top_p\",\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 1000\n",
    "    },\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    input_shields=[],\n",
    "    output_shields=[],\n",
    "    enable_session_persistence=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "print(f\"Session Id: {session_id} with the Agent {agent.agent_id} created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Was anything related to Llama3 discussed? If so, what was that?\",\n",
    "    \"Tell me how to use LoRA?\",\n",
    "    \"What about Quantization?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "    print(f\"User ... > {prompt}\")\n",
    "\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
