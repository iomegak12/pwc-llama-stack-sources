{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KeM7OsiCu81"
      },
      "outputs": [],
      "source": [
        "%pip install -U transformers datasets accelerate peft trl bitsandbytes wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "import os, torch, wandb\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, setup_chat_format"
      ],
      "metadata": {
        "id": "_xgN71rFD-Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token = hf_token)\n",
        "\n",
        "wb_token = userdata.get(\"WAND_TOKEN\")\n",
        "wandb.login(key=wb_token)\n",
        "\n",
        "run = wandb.init(\n",
        "    project='Fine-tune Llama 3 8B on Medical Dataset',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "id": "czg2o6ZSEHQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Llama-3.1-8B\"\n",
        "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
        "new_model = \"llama-3-8b-chat-doctor\""
      ],
      "metadata": {
        "id": "d-kek-12EZmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_dtype = torch.float16\n",
        "attn_implementation = \"eager\""
      ],
      "metadata": {
        "id": "ftrnL0M_EuEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=attn_implementation\n",
        ")"
      ],
      "metadata": {
        "id": "Gn_qhG7QE1ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "E4xR0xvkFLAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "LRFFZIC4FPUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split=\"all\")\n",
        "dataset = dataset.shuffle(seed=65).select(range(1000))\n",
        "\n",
        "def format_chat_template(row):\n",
        "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
        "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "dataset['text'][3]"
      ],
      "metadata": {
        "id": "qQIRY8FbFUJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "6tdxxijUFbm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=new_model,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ],
      "metadata": {
        "id": "l38Ka2dQFeVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "a0Wt17lpFiYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GloxnDDmFkzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "fnhU95SlHerl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it? Please provide the solution in 2 lines.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True,\n",
        "                   truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=150,\n",
        "                         num_return_sequences=1)\n",
        "\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(text.split(\"assistant\")[1])"
      ],
      "metadata": {
        "id": "mB1yR_ffHjZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(new_model)\n",
        "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "YFr4xFO_H-Gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}