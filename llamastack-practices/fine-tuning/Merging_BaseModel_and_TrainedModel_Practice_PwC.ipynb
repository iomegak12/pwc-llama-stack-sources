{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzOADHhaJFaq"
      },
      "outputs": [],
      "source": [
        "%pip install -U bitsandbytes transformers accelerate peft trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token = hf_token)"
      ],
      "metadata": {
        "id": "01s7a69UJuos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Llama-3.1-8B\"\n",
        "new_model = \"/content/drive/MyDrive/llama-3-8b-chat-doctor/\""
      ],
      "metadata": {
        "id": "qGsJJaq2Jx9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from trl import setup_chat_format\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        return_dict=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "\n",
        "base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "-shlz22uJ4IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"}]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "outputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "0eVnKbR-KbIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"llama-3-8b-chat-doctor-full\")\n",
        "tokenizer.save_pretrained(\"llama-3-8b-chat-doctor-full\")"
      ],
      "metadata": {
        "id": "HZgylBfkKfvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"llama-3-8b-chat-doctor-full\", use_temp_dir=False)\n",
        "tokenizer.push_to_hub(\"llama-3-8b-chat-doctor-full\", use_temp_dir=False)"
      ],
      "metadata": {
        "id": "yC5X0cZDK1xs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}