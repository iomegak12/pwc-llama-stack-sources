{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching Llama Stack Server from Local and Remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER_HOST = os.environ[\"LLAMA_STACK_SERVER_HOST\"]\n",
    "LLAMA_STACK_SERVER_PORT = os.environ[\"LLAMA_STACK_SERVER_PORT\"]\n",
    "\n",
    "LLAMA_STACK_SERVER_REMOTE_HOST = os.environ[\"LLAMA_STACK_SERVER_REMOTE_HOST\"]\n",
    "LLAMA_STACK_SERVER_REMOTE_PORT = os.environ[\"LLAMA_STACK_SERVER_REMOTE_PORT\"]\n",
    "\n",
    "local_client = LlamaStackClient(base_url=f'http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}')\n",
    "cloud_client = LlamaStackClient(base_url=f'http://{LLAMA_STACK_SERVER_REMOTE_HOST}:{LLAMA_STACK_SERVER_REMOTE_PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from termcolor import cprint\n",
    "\n",
    "async def select_client(use_local: bool) -> LlamaStackClient:\n",
    "    if use_local:\n",
    "        return local_client\n",
    "\n",
    "    if not use_local:\n",
    "        return cloud_client\n",
    "\n",
    "client = await select_client(use_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import cprint\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "INFERENCE_MODEL = os.environ[\"INFERENCE_MODEL\"]\n",
    "\n",
    "async def get_llama_response(stream: bool = True, use_local: bool = True):\n",
    "    client = await select_client(use_local)  # Selects the available client\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": 'hello world, write me a 2 sentence poem about the moon'\n",
    "    }\n",
    "    \n",
    "    cprint(f'User> {message[\"content\"]}', 'green')\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=INFERENCE_MODEL,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    if not stream:\n",
    "        cprint(f'> Response: {response.completion_message.content}', 'cyan')\n",
    "    else:\n",
    "        async for log in EventLogger().log(response):\n",
    "            log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> hello world, write me a 2 sentence poem about the moon\u001b[0m\n",
      "\u001b[36m> Response: Here is a 2-sentence poem about the moon:\n",
      "\n",
      "The moon glows bright in the midnight sky,\n",
      "A silver crescent, catching the eye.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Run this function directly in a Jupyter Notebook cell with `await`\n",
    "await get_llama_response(stream=False, use_local=False)\n",
    "\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(get_llama_response(use_local=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> hello world, write me a 2 sentence poem about the moon\u001b[0m\n",
      "\u001b[36m> Response: Here is a 2-sentence poem about the moon:\n",
      "\n",
      "The moon glows bright in the midnight sky,\n",
      "A silver crescent, catching the eye.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Run this function directly in a Jupyter Notebook cell with `await`\n",
    "await get_llama_response(stream=False, use_local=True)\n",
    "\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(get_llama_response(use_local=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
